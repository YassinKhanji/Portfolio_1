{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import datetime as dt\n",
    "import os\n",
    "from functools import reduce\n",
    "from fetch_symbols import get_symbols\n",
    "\n",
    "\n",
    "class Data:\n",
    "    def __init__(self, symbols, interval, start_time, end_time):\n",
    "        self.symbols = symbols\n",
    "        self.interval = interval\n",
    "        self.start_time = start_time\n",
    "        self.end_time = end_time\n",
    "        self.available_symbols = self.kraken_symbols()\n",
    "        self.df = self.get_data()\n",
    "\n",
    "    # def binance_symbols(self):\n",
    "    #     \"\"\"Fetch available symbols from Binance API.\"\"\"\n",
    "    #     response = requests.get(\"https://api.binance.com/api/v3/exchangeInfo\")\n",
    "    #     exchange_info = response.json()\n",
    "    #     valid_symbols = {s['symbol'] for s in exchange_info['symbols']}\n",
    "    #     return [s for s in self.symbols if s in valid_symbols]\n",
    "    \n",
    "    def kraken_symbols(self):\n",
    "        \"\"\"Fetch available symbols from Kraken API.\"\"\"\n",
    "        response = requests.get(\"https://api.kraken.com/0/public/AssetPairs\")\n",
    "        asset_pairs = response.json()['result']\n",
    "        valid_symbols = {pair for pair in asset_pairs.keys()}\n",
    "        return [s for s in self.symbols if s in valid_symbols]\n",
    "\n",
    "\n",
    "    def fetch_symbol_data(self, symbol, date_list, url, limit):\n",
    "        \"\"\"Fetch kline data for a single symbol.\"\"\"\n",
    "        all_data = []\n",
    "        for i in range(len(date_list) - 1):\n",
    "            params = {\n",
    "                'symbol': symbol,\n",
    "                'interval': self.interval,\n",
    "                'startTime': int(date_list[i].timestamp() * 1000),\n",
    "                'endTime': int((date_list[i + 1] - dt.timedelta(seconds=1)).timestamp() * 1000),\n",
    "                'limit': limit,\n",
    "            }\n",
    "            response = requests.get(url, params=params)\n",
    "            data = response.json()\n",
    "            if isinstance(data, list):\n",
    "                all_data.extend(data)\n",
    "        return symbol, all_data\n",
    "\n",
    "    def get_binance_klines(self, limit=1000):\n",
    "        \"\"\"Fetch historical kline data for all symbols in parallel.\"\"\"\n",
    "        url = \"https://api.binance.com/api/v3/klines\"\n",
    "        date_list = pd.date_range(start=self.start_time, end=self.end_time, freq='D').tolist()\n",
    "\n",
    "        # Use ThreadPoolExecutor for parallel fetching\n",
    "        with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "            results = executor.map(\n",
    "                lambda symbol: self.fetch_symbol_data(symbol, date_list, url, limit),\n",
    "                self.available_symbols,\n",
    "            )\n",
    "\n",
    "        # Process and combine results\n",
    "        data_frames = {}\n",
    "        for symbol, data in results:\n",
    "            if not data:\n",
    "                continue\n",
    "            df = pd.DataFrame(data)\n",
    "            df = df.iloc[:, 0:6]\n",
    "            df.columns = ['Open Time', 'open', 'high', 'low', 'close', 'volume']\n",
    "            df.index = pd.to_datetime(df['Open Time'], unit='ms')\n",
    "            df.drop('Open Time', axis=1, inplace=True)\n",
    "            data_frames[symbol] = df\n",
    "\n",
    "        if not data_frames:\n",
    "            return None\n",
    "\n",
    "        combined_df = pd.concat(data_frames, axis=1)\n",
    "        combined_df = combined_df.swaplevel(axis=1).sort_index(axis=1)\n",
    "        combined_df = combined_df.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "        return combined_df\n",
    "\n",
    "    def prepare_data(self, df):\n",
    "        \"\"\"Prepare data for analysis.\"\"\"\n",
    "        df = df.copy()\n",
    "        for coin in df.columns.levels[1]:\n",
    "            df['returns', coin] = df['close', coin].pct_change()\n",
    "            df['log_return', coin] = np.log(df['returns', coin] + 1)\n",
    "            df[\"creturns\", coin] = df[\"log_return\", coin].cumsum().apply(np.exp)\n",
    "            df['price', coin] = df['close', coin]\n",
    "            df['volume_in_dollars', coin] = df['close', coin] * df['volume', coin]\n",
    "\n",
    "        df = df.stack(future_stack=True)\n",
    "        df.sort_index(axis=1, inplace=True)\n",
    "        df.index.names = ['date', 'coin']\n",
    "        df.dropna(inplace=True)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def upload_data(self, df, filename):\n",
    "        \"\"\"Save data to a CSV file.\"\"\"\n",
    "        df.to_csv(filename)\n",
    "\n",
    "    def get_data(self):\n",
    "        \"\"\"Main function to fetch, prepare, and save data.\"\"\"\n",
    "        df = self.get_binance_klines()\n",
    "        if df is not None:\n",
    "            df = self.prepare_data(df)\n",
    "            self.upload_data(df, 'data.csv')\n",
    "        return df\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "class CSV_Data:\n",
    "    def __init__(self, folder_path, symbols):\n",
    "        self.folder_path = folder_path\n",
    "        self.symbols = symbols\n",
    "        self.df = self.process_folder(folder_path, symbols)\n",
    "        self.df = self.prepare_data()\n",
    "        self.upload_data_to_csv(self.df)\n",
    "        \n",
    "    \n",
    "    def prepare_data(self):\n",
    "        df = self.df.copy()\n",
    "        for coin in df.columns.levels[1]:\n",
    "            df['returns', coin] = df['close', coin].pct_change()\n",
    "            df['log_return', coin] = np.log(df['returns', coin])\n",
    "            df[\"creturns\", coin] = df[\"log_return\", coin].cumsum().apply(np.exp)\n",
    "            df['price', coin] = df['close', coin]\n",
    "            df['volume_in_dollars', coin] = df['close', coin] * df['volume', coin]\n",
    "\n",
    "        df = df.stack(level=1, future_stack=True)\n",
    "        df.sort_index(axis=1, inplace=True)\n",
    "        df.index.names = ['date', 'coin']\n",
    "        df.dropna(inplace=True)\n",
    "\n",
    "        return df\n",
    "    \n",
    "    def get_data(self, file_path, symbols):\n",
    "        df = pd.read_csv(file_path)\n",
    "        df = df.drop(columns = df.columns[-1]).reset_index()\n",
    "        df.drop(columns = df.columns[0], inplace = True)\n",
    "        df.drop(index = 0, inplace = True)\n",
    "        df.columns = ['date', 'coin', 'open', 'high', 'low', 'close', 'volume', 'volume_in_dollars']\n",
    "\n",
    "        if not df['coin'].iloc[0] in symbols:\n",
    "            return\n",
    "        # Clean the date column by stripping whitespace\n",
    "        df['date'] = df['date'].str.strip()\n",
    "        # Parse the date column with mixed format\n",
    "        df['date'] = pd.to_datetime(df['date'], format='mixed', errors='coerce')\n",
    "        \n",
    "        df.set_index([df.columns[0], df.columns[1]], inplace = True)\n",
    "        df = df.unstack()\n",
    "        return df\n",
    "    \n",
    "    def process_folder(self, folder_path, symbols):\n",
    "        # Get all CSV files in the folder\n",
    "        csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "        \n",
    "        dfs = []\n",
    "        \n",
    "        for file in csv_files:\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            df = self.get_data(file_path, symbols)\n",
    "            if df is not None:\n",
    "                dfs.append(df)\n",
    "        \n",
    "\n",
    "        # Get the union of all indices (dates) to align the data\n",
    "        all_dates = reduce(pd.Index.union, [df.index.get_level_values(0) for df in dfs])\n",
    "\n",
    "        # Reindex all DataFrames to the same set of dates (adding NaNs where data is missing)\n",
    "        dfs_aligned = [df.reindex(all_dates, level=0, fill_value=None) for df in dfs]\n",
    "\n",
    "        # Concatenate all DataFrames\n",
    "        concatenated_df = pd.concat(dfs_aligned, axis=1)\n",
    "        concatenated_df = concatenated_df.sort_index(axis=1)\n",
    "        concatenated_df = concatenated_df.apply(pd.to_numeric, errors='coerce', downcast='float') #Essential to perform calculations\n",
    "        \n",
    "        return concatenated_df\n",
    "\n",
    "    def upload_data_to_csv(self, df):\n",
    "        # Upload the data to CSV file\n",
    "        df.to_csv('all_data.csv')\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "symbols = get_symbols()\n",
    "interval = '1h'\n",
    "start_time = dt.datetime(2020, 1, 1)\n",
    "end_time = dt.datetime(2020, 2, 1)  \n",
    "df = Data(symbols, interval, start_time, end_time).df\n",
    "print(df)\n",
    "\n",
    "\n",
    "#Use the below for uploading full data (uploaded to csv)\n",
    "# binance_symbols = Data(symbols)\n",
    "# folder_path = r'C:\\Users\\yassi\\OneDrive\\Documents\\Trading\\Algo Trading Projects\\Algo Business\\data\\Binance Data (CSV)'\n",
    "# df = CSV_Data(folder_path, symbols).df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
